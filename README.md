# machine-learning-Concepts
machine learning concepts

## Transformer

The transformer includes following modules:
1. SelfAttention
2. MultiheadAttention
3. PositionalEncoding
4. EncoderBlock
5. DecoderBlock
6. TransformerBlock

## MixtureOfExperts


References and additional links:
Sparse Mixture of Experts paper: https://arxiv.org/abs/1701.06538
Mixtral of Experts: https://arxiv.org/abs/2401.04088
DeepSeek V2: https://arxiv.org/abs/2405.04434
DeepSeek V3: https://arxiv.org/abs/2412.19437
Switch Transformers / Expert Capacity:  https://arxiv.org/abs/2101.03961
A Blog post: https://brunomaga.github.io/Mixture-o...
A visual guide: https://newsletter.maartengrootendors...
Survey paper: https://arxiv.org/pdf/2407.06204
